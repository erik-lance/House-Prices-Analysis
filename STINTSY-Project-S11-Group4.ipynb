{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 1. Introduction to the problem/task and dataset\n",
    "The dataset contains house prices for King County which is located in the US State of Washington. It includes homes sold between May 2014 and May 2015. It is an IBM dataset that focused on predicting the prices of houses in the USA through analysis.\n",
    "\n",
    "In the realm of real estate and housing, the condition of a property plays a pivotal role in its market value. Understanding and accurately assessing the condition of houses is essential for buyers, sellers, and real estate professionals alike.\n",
    "\n",
    "To address this need, we embark on a project aimed at classifying houses based on their condition. The condition of a house, graded on a scale of 1 to 5, serves as our target variable. This classification task will empower us to predict and differentiate houses based on their state of repair and maintenance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 2. Description of the dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- https://rstudio-pubs-static.s3.amazonaws.com/155304_cc51f448116744069664b35e7762999f.html -->\n",
    "<!-- https://www.kaggle.com/datasets/harlfoxem/housesalesprediction -->\n",
    "\n",
    "The dataset of this project encompasses a comprehensive collection of housing records, each providing insights into the conditions and attributes of residential properties. It serves as the foundation for our task of classifying houses based on their condition. This dataset has been meticulously assembled from multiple sales transactions, capturing houses that have changed ownership over time.\n",
    "\n",
    "The data for these sales comes from the official public records of home sales in the King County area, Washington State. The data sets contains 21613 rows. Each represents a home sold from May 2014 through May 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is structured as a single file in the widely-used CSV (Comma-Separated Values) format. Each row in the dataset represents a distinct house sale event, while each column corresponds to an attribute or feature of the property.\n",
    "\n",
    "In total, the dataset comprises:\n",
    "- `21613` instances; and\n",
    "- `21` features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief Description of Features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our dataset encompasses a rich array of features, both numerical and categorical, each contributing to our understanding of the condition and characteristics of houses. Below is a list of the features included in the dataset, grouped by relevance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Location**\n",
    "- `lat` and `long` represent the latitude and longitude of the house's location.\n",
    "\n",
    "**Size:**\n",
    "- `sqft_living` is the square footage of the interior living space.\n",
    "- `sqft_lot` is the square footage of the land.\n",
    "- `sqft_living15` is the square footage of interior living space for the nearest 15 neighbors.\n",
    "- `sqft_lot15` is the square footage of the land lots of the nearest 15 neighbors.\n",
    "\n",
    "**Rooms:**\n",
    "- `bedrooms` counts the number of bedrooms in the house.\n",
    "- `bathrooms` counts the number of bathrooms. A value of .5 indicates a room with a toilet but no shower.\n",
    "\n",
    "**Floors:**\n",
    "- `floors` is the number of floors in the house.\n",
    "\n",
    "**Waterfront and View:**\n",
    "- `waterfront` is a binary variable, indicating whether the house overlooks the waterfront (1 for yes, 0 for no).\n",
    "- `view` is an index from 0 to 4, rating the quality of the property's view.\n",
    "\n",
    "**Condition and Grade:**\n",
    "- `condition` is an index from 1 to 5, indicating the condition of the apartment.\n",
    "- `grade` is an index from 1 to 13, where 1-3 represent lower-quality construction, 7 indicates average quality, and 11-13 signify high-quality construction and design.\n",
    "\n",
    "**Square Footage Above and Below Ground:**\n",
    "- `sqft_above` represents the square footage of the interior housing space above ground level.\n",
    "- `sqft_basement` represents the square footage of the interior housing space below ground level.\n",
    "\n",
    "**Year Information:**\n",
    "- `yr_built` is the year the house was initially built.\n",
    "- `yr_renovated` is the year of the last house renovation.\n",
    "\n",
    "**Location:**\n",
    "- `zipcode` indicates the zipcode area where the house is situated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These attributes provide quantitative details about the properties. In contrast, categorical data, such as 'waterfront' and 'view,' offer qualitative information about specific aspects of the houses. It's important to clarify the significance of each feature as it guides our analysis and classification process. Even those features not directly utilized in our study may hold relevance for a comprehensive understanding of housing conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full Feature Table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature        | Description                                                                             |\n",
    "|----------------|-----------------------------------------------------------------------------------------|\n",
    "| id             | Unique ID for each home sold                                                            |\n",
    "| date           | Date of the home sale                                                                  |\n",
    "| price          | Price of each home sold                                                               |\n",
    "| bedrooms       | Number of bedrooms                                                                    |\n",
    "| bathrooms      | Number of bathrooms, where .5 accounts for a room with a toilet but no shower         |\n",
    "| sqft_living    | Square footage of the apartment's interior living space                               |\n",
    "| sqft_lot       | Square footage of the land space                                                       |\n",
    "| floors         | Number of floors                                                                      |\n",
    "| waterfront     | A dummy variable for whether the apartment was overlooking the waterfront or not     |\n",
    "| view           | An index from 0 to 4 of how good the view of the property was                          |\n",
    "| condition      | An index from 1 to 5 on the condition of the apartment                                 |\n",
    "| grade          | An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design |\n",
    "| sqft_above     | The square footage of the interior housing space that is above ground level           |\n",
    "| sqft_basement  | The square footage of the interior housing space that is below ground level           |\n",
    "| yr_built       | The year the house was initially built                                                 |\n",
    "| yr_renovated   | The year of the houseâ€™s last renovation                                                |\n",
    "| zipcode        | What zipcode area the house is in                                                     |\n",
    "| lat            | Latitude                                                                              |\n",
    "| long           | Longitude                                                                             |\n",
    "| sqft_living15  | The square footage of interior housing living space for the nearest 15 neighbors     |\n",
    "| sqft_lot15     | The square footage of the land lots of the nearest 15 neighbors                         |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 3. List of Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Seed for reproducibility\n",
    "np.random.seed(42)\n",
    "SEED = 42\n",
    "\n",
    "# Preprocessing\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Geological Visualization\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point, Polygon\n",
    "\n",
    "# Machine Learning\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import learning_curve\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Evaluation\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score, f1_score, precision_score, recall_score, log_loss\n",
    "\n",
    "# Hyperparameter Tuning\n",
    "from sklearn.model_selection import RandomizedSearchCV, GridSearchCV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 4. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we intend to classify houses based on their condition, we want to remove biases that may arise from duplicate data. We will remove duplicate rows from the dataset. In this case, there are duplicate houses due to the fact that some houses were sold more than once during the period of study. We will remove the duplicates and keep the last instance of the house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('house_prices.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing Duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_old_rows = len(df)\n",
    "\n",
    "# Store rows with duplicate ids in a separate dataframe\n",
    "df_duplicates = df[df.duplicated(['id'], keep=False)]\n",
    "\n",
    "# Store rows from df_duplicates tha are not the most recent sale in a separate dataframe\n",
    "df_duplicates = df_duplicates[df_duplicates.duplicated(['id'], keep='last')]\n",
    "\n",
    "# Remove rows from df that are in df_duplicates\n",
    "df = df.drop(df_duplicates.index)\n",
    "\n",
    "# Print number old and new number of rows\n",
    "print(f'Old Rows: {df_old_rows}\\nNew Rows: {len(df)}\\nRemoved Rows: {len(df_duplicates)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm there are no more duplicate ids\n",
    "df[df.duplicated(['id'], keep=False)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Cleaning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now clean the data by checking for missing values or incorrect data types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for string values in the dataframe\n",
    "df.select_dtypes(include=['object']).columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values in the dataset\n",
    "df.isnull().sum().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Negative Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for negative values (excluding longitude, latitude, date, and id)\n",
    "excluded_cols = ['id', 'long', 'lat', 'date']\n",
    "df[(df.drop(excluded_cols, axis=1) < 0).any(axis=1)]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is a date column. We will extract the `year`, `month`, `day`, and `day_of_week` from the date column to see if this information is useful for our classification task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date to datetime format\n",
    "df['date'] = pd.to_datetime(df['date'], format='%Y%m%dT%H%M%S')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract features from date\n",
    "df['year_sold'] = df['date'].dt.year\n",
    "df['month_sold'] = df['date'].dt.month\n",
    "df['day_sold'] = df['date'].dt.day\n",
    "df['day_of_week_sold'] = df['date'].dt.dayofweek\n",
    "\n",
    "# One-hot encode day_of_week\n",
    "df = pd.get_dummies(df, columns=['day_of_week_sold'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 5. Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Summary and Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Breakdown"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a summary of the dataset\n",
    "# Remove columns with no significant data and format the output\n",
    "remove_cols = ['id', 'date']\n",
    "df_summary = df.drop(remove_cols, axis=1).describe().transpose()\n",
    "df_summary = df_summary[['mean', 'std', 'min', '25%', '50%', '75%', 'max']]\n",
    "df_summary = df_summary.round(2)\n",
    "df_summary = df_summary.rename(columns={'mean': 'Mean', 'std': 'Standard Deviation', 'min': 'Minimum', '25%': '25th Percentile', '50%': '50th Percentile', '75%': '75th Percentile', 'max': 'Maximum'})\n",
    "df_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### House Prices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now look at the numerical distribution of house prices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the numerical data behind `price`\n",
    "df['price'].describe().apply(lambda x: format(x, 'f'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The average price for a house appears to be `$541,649.9627` with a standard deviation of `$367,314.9294`. The minimum price for a house is `$75,000.0000` and the maximum price for a house is `$7,700,000.0000`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### House Built and Renovated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now know that most houses are in average condition. Let's see how many houses were built and renovated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new figure with two subplots\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))  # 1 row, 2 columns\n",
    "\n",
    "# Plot the 'Year Built' histogram on the first subplot (ax1)\n",
    "ax1.hist(df['yr_built'], edgecolor='k')\n",
    "ax1.set_title('Year Built')\n",
    "ax1.set_xlabel('Year Built')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "# Plot the 'Year Renovated' histogram on the second subplot (ax2)\n",
    "renovated = df[df['yr_renovated'] > 0]\n",
    "ax2.hist(renovated['yr_renovated'], edgecolor='k')\n",
    "ax2.set_title('Year Renovated')\n",
    "ax2.set_xlabel('Year Renovated')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "# Adjust the layout to prevent overlap\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the combined plot\n",
    "plt.show()\n",
    "\n",
    "# Print how many houses have been renovated and how many have not\n",
    "print(f'Renovated: {len(renovated)}\\nNot Renovated: {len(df) - len(renovated)}')\n",
    "\n",
    "print('\\n')\n",
    "\n",
    "# Print the year most houses were built and the number of houses built in that year\n",
    "print(f'Most houses were built in {df[\"yr_built\"].mode()[0]}')\n",
    "print(f'Number of houses built in {df[\"yr_built\"].mode()[0]}: {len(df[df[\"yr_built\"] == df[\"yr_built\"].mode()[0]])}')\n",
    "\n",
    "# Print the year most houses were renovated and the number of houses renovated in that year (don't include houses that haven't been renovated)\n",
    "print(f'Most houses were renovated in {renovated[\"yr_renovated\"].mode()[0]}')\n",
    "print(f'Number of houses renovated in {renovated[\"yr_renovated\"].mode()[0]}: {len(renovated[renovated[\"yr_renovated\"] == renovated[\"yr_renovated\"].mode()[0]])}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the data, it seems that the renovated houses are not that many. Meaning that most houses are in their original state. Most houses appear to be built and renovated in 2014. This means that most houses are new and have not been renovated yet. However, we must consider that the data was collected from May 2014 to May 2015. This may have underlying effects on the data due to the fact that the data was collected in a span of one year and that 2014 is the most recent year in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### House material grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The material grade of a house is an important factor in determining the condition and price of a house. Let's see how many houses are in each material grade.\n",
    "\n",
    "`grade` - \n",
    "*An index from 1 to 13, where 1-3 falls short of building construction and design, 7 has an average level of construction and design, and 11-13 have a high quality level of construction and design*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the grade distribution\n",
    "plt.hist(df['grade'], bins=13, edgecolor='k')\n",
    "plt.title('Grade')\n",
    "plt.xlabel('Grade')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Print the value counts for `grade`\n",
    "df['grade'].value_counts()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most houses appear to be in grade 7. This means that most houses have an average level of construction and design. This means that most houses are not that well-built and designed. This may be a factor in determining the condition of a house.\n",
    "\n",
    "- Only four (4) houses are in grade 1-3 meaning that only four (4) houses are poorly built and designed and in poor condition.\n",
    "- `498` houses are in grade 11-13 meaning that `498` houses are well-built and designed and in good condition.\n",
    "- `16,554` houses are in grade 4-10 meaning that `16,554` houses are averagely built and designed and in average condition.\n",
    "  - `2047` houses are in grade 4-6 meaning that `2047` houses are below average condition.\n",
    "  - `8896` houses are in grade 7 meaning that `8896` houses are average condition.\n",
    "  - `6611` houses are in grade 8-10 meaning that `6611` houses are above average condition.\n",
    "\n",
    "The construction of most houses appear to be on the higher spectrum of material grade. It is more likely that most houses are in average to above average condition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Rooms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's have a quick look at the number of bedrooms and bathrooms in houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot bedrooms and bathrooms on separate plots as a histogram\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))  # 1 row, 2 columns\n",
    "\n",
    "ax1.hist(df['bedrooms'], edgecolor='k')\n",
    "ax1.set_title('Bedrooms')\n",
    "ax1.set_xlabel('Bedrooms')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "ax2.hist(df['bathrooms'], edgecolor='k')\n",
    "ax2.set_title('Bathrooms')\n",
    "ax2.set_xlabel('Bathrooms')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Describe bedrooms and bathrooms\n",
    "df[['bedrooms', 'bathrooms']].describe()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon observation, it is common for houses to have `three (3)` bedrooms and `two (2)` bathrooms. This may infer that most houses are built for families. Containing possibly at least three persons."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Square Feet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, let's have a look at the square feet of houses. This may help us determine the size of houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the `sqft_lot` and `sqft_living` columns on separate plots as a histogram\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 4))  # 1 row, 2 columns\n",
    "\n",
    "ax1.hist(df['sqft_lot'], edgecolor='k')\n",
    "ax1.set_title('Square Feet Lot')\n",
    "ax1.set_xlabel('Square Feet Lot')\n",
    "ax1.set_ylabel('Count')\n",
    "\n",
    "ax2.hist(df['sqft_living'], edgecolor='k')\n",
    "ax2.set_title('Square Feet Living')\n",
    "ax2.set_xlabel('Square Feet Living')\n",
    "ax2.set_ylabel('Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Describe `sqft_lot` and `sqft_living` and format the output. Add another column for the percentage difference between the two\n",
    "sqft = df[['sqft_lot', 'sqft_living']].describe()\n",
    "sqft['difference'] = sqft['sqft_lot'] - sqft['sqft_living']\n",
    "sqft['difference_percentage'] = sqft['difference'] / sqft['sqft_lot']\n",
    "sqft = sqft.round(2)\n",
    "sqft = sqft.rename(columns={'sqft_lot': 'Square Feet Lot', 'sqft_living': 'Square Feet Living'})\n",
    "sqft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, houses have a square feet of `2082.70` and a lot size of `15135.64`. We observe that there is about `86%` of the lot that goes unused. However, for lower lot sizes, it's around `44%`. This possibly infers that the smaller the lot size, the more likely it is that the lot is fully utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the summary of the various features, we can infer the following:\n",
    "- Most houses are in average condition and it is more likely that most houses are in average to above average condition.\n",
    "- `22.56%` of the houses have renovated at least once. It is more likely that most houses are in their original state.\n",
    "- Most houses are in grade 7 meaning that most houses have an average level of construction and design. It is also more likely for houses to have grade 7 and higher, meaning houses are in average to above average in terms of construction quality.\n",
    "- Most houses have `three (3)` bedrooms and `two (2)` bathrooms. It is more likely that most houses are built for families.\n",
    "- We observe that commonly, there is about `86%` of the lot that goes unused. However, for lower lot sizes, it's around `44%`. This possibly infers that the smaller the lot size, the more likely it is that the lot is fully utilized."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Class Distribution Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Condition of Houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's start by finding out what are the number of houses in each condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histogram of the `condition` column with labels\n",
    "plt.hist(df['condition'], bins=5, edgecolor='k')\n",
    "plt.title('Condition')\n",
    "plt.xlabel('Condition')\n",
    "plt.ylabel('Count')\n",
    "plt.show()\n",
    "\n",
    "# Also print the value counts\n",
    "df['condition'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get percentage of the houses based on condition\n",
    "condition_percentages = df['condition'].value_counts(normalize=True)\n",
    "\n",
    "# Format the percentages to 2 decimal places\n",
    "condition_percentages.map(lambda x: '{:.2f}%'.format(x*100))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On average, most houses would fall under the 3.0 category. This means that most houses are in average condition. Meaning, out of `21436` houses, `13911` houses are in average condition. This means that `64.90%` of the houses are in average condition.\n",
    "\n",
    "As for those with above average houses, houses under 4.0 and 5.0 are considered above average. This means that out of `21436` houses, `7332` houses are above average. This means that `34.20%` of the houses are above average.\n",
    "- There are `5646` houses with a condition of four (4). They comprise `26.33%` of the houses.\n",
    "- There are `1687` houses with a condition of five (5). They comprise `7.87%` of the houses.\n",
    "\n",
    "As for those with below average houses, houses under 1.0 and 2.0 are considered below average. This means that out of `21436` houses, `193` houses are below average. This means that `0.9%` of the houses are below average.\n",
    "- There are `164` houses with a condition of two (2). They comprise `0.77%` of the houses.\n",
    "- There are `29` houses with a condition of one (1). They comprise `0.14%` of the houses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correlation Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are classifying houses based on their condition, we want to know which features are correlated to the condition of a house. This will help us determine which features are important in determining the condition of a house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Price"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Price is the most common determinant of quality in products. It is common belief in most people that the more expensive a product is, the better the quality. This may also be true for houses. Let's see if the price of a house is correlated to its condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "\n",
    "# Original Scatter plot\n",
    "axes[0].scatter(df['condition'], df['price'])\n",
    "axes[0].set_title('Condition vs Price')\n",
    "axes[0].set_xlabel('Condition')\n",
    "axes[0].set_ylabel('Price')\n",
    "\n",
    "# Add a correlation line to the first scatter plot (axes[0])\n",
    "correlation_line = np.polyfit(df['condition'], df['price'], 1)\n",
    "axes[0].plot(df['condition'], np.polyval(correlation_line, df['condition']), color='orange', linestyle='dashed', linewidth=2)\n",
    "\n",
    "# Get correlation coefficient for condition and price\n",
    "correlation_coefficient = np.corrcoef(df['condition'], df['price'])[0, 1]\n",
    "print(f'Correlation Coefficient: {correlation_coefficient}')\n",
    "\n",
    "# Calculate median price for each condition\n",
    "median_prices = df.groupby('condition')['price'].median().reset_index(name='median_price')\n",
    "\n",
    "# Scatter plot with normalized median prices and a connecting line\n",
    "axes[1].scatter(median_prices['condition'], median_prices['median_price'])\n",
    "axes[1].plot(median_prices['condition'], median_prices['median_price'], color='red', linestyle='dashed', marker='o', markersize=8)\n",
    "axes[1].set_title('Condition vs Median Price with Trend Line')\n",
    "axes[1].set_xlabel('Condition')\n",
    "axes[1].set_ylabel('Median Price')\n",
    "\n",
    "# Calculate mean price for each condition\n",
    "mean_prices = df.groupby('condition')['price'].mean().reset_index(name='mean_price')\n",
    "\n",
    "# Scatter plot with normalized mean prices and a connecting line\n",
    "axes[2].scatter(mean_prices['condition'], mean_prices['mean_price'])\n",
    "axes[2].plot(mean_prices['condition'], mean_prices['mean_price'], color='red', linestyle='dashed', marker='o', markersize=8)\n",
    "axes[2].set_title('Condition vs Mean Price with Trend Line')\n",
    "axes[2].set_xlabel('Condition')\n",
    "axes[2].set_ylabel('Mean Price')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the price of a house is not correlated to its condition. However, the more expensive a house is based on average values, the better its condition. This may be due to the fact that the more expensive a house is, the more likely it is that the house is well-built and designed. We see that the houses with lower conditions reach the smaller price ranges. But again, we can not confirm that it is directly correlated to the condition.\n",
    "\n",
    "However, oddly enough, the best conditioned houses do not have the highest price, but on average are more expensive. It was the houses with the condition at 4.0 that got the highest price, probably due to some outliers. The correlation is not perfectly linear, however, it can  still be observed that the prices are indeed higher for conditions 3+ in contrast to 1 and 2.\n",
    "\n",
    "Judging by the polyfit line on the scatter plot, the correlation is quite straight and with a correlation value of 0.03453, we can say that the price of a house is not correlated to its condition. However, we can say that the average prices of each condition reveal that the price of conditions 1 and 2 are around 350,000 and 100,000 respectively, while the price of conditions 3, 4, and 5 are around 550,000, 525,000, and 700,000 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Grade"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see if the grade of a house is correlated to its condition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(12, 4))\n",
    "\n",
    "# Original Violin plot\n",
    "sns.violinplot(x='condition', y='grade', data=df, palette='viridis', ax=axes[0])\n",
    "axes[0].set_title('Condition vs Grade (Violin Plot)')\n",
    "axes[0].set_xlabel('Condition')\n",
    "axes[0].set_ylabel('Grade')\n",
    "\n",
    "# Add a correlation line to the first plot (axes[0])\n",
    "correlation_line = np.polyfit(df['condition'], df['grade'], 1)\n",
    "axes[0].plot(df['condition'], np.polyval(correlation_line, df['condition']), color='orange', linestyle='dashed', linewidth=2)\n",
    "\n",
    "# Get correlation coefficient for condition and grade\n",
    "correlation_coefficient = np.corrcoef(df['condition'], df['grade'])[0, 1]\n",
    "print(f'Correlation Coefficient: {correlation_coefficient}')\n",
    "\n",
    "# Calculate median grade for each condition\n",
    "median_grades = df.groupby('condition')['grade'].median().reset_index(name='median_grade')\n",
    "\n",
    "# Original KDE plot\n",
    "sns.kdeplot(data=df, x='grade', hue='condition', common_norm=False, fill=True, palette='viridis', ax=axes[1])\n",
    "axes[1].set_title('Condition vs Grade (KDE Plot)')\n",
    "axes[1].set_xlabel('Grade')\n",
    "axes[1].set_ylabel('Density')\n",
    "\n",
    "# Scatter plot with normalized median grades and a connecting line\n",
    "axes[2].scatter(median_grades['condition'], median_grades['median_grade'])\n",
    "axes[2].plot(median_grades['condition'], median_grades['median_grade'], color='red', linestyle='dashed', marker='o', markersize=8)\n",
    "axes[2].set_title('Condition vs Grade with Median Line (Scatter Plot)')\n",
    "axes[2].set_xlabel('Condition')\n",
    "axes[2].set_ylabel('Grade')\n",
    "\n",
    "# Adjust layout\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grade of a house does not have a good correlation to its condition with a value of -0.1482.\n",
    "\n",
    "The higher the grade of a house, the better its condition. This may be due to the fact that the higher the grade of a house, the more likely it is that the house is well-built and designed. We see that the houses with lower conditions reach the lower grade ranges.\n",
    "\n",
    "Only with condition 1 is where you can find a house with a grade of 1. It's highest is only 8 when the other conditions can reach 12 and 13. With a large majority of houses being condition 3, it is not surprising to see that it was able to span a lot of grades but nothing below 3."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we have observed, the correlation of values seem to be difficult due to the small sample size of the data on condition 1, 2, and 5. However, we can still observe some correlations. Let's see if we can find any correlations between the features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the correlation of condition to every other feature only\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(df.corr()[['condition']].sort_values('condition'), annot=True, cmap='viridis')\n",
    "plt.title('Correlation of Condition to Every Other Feature')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears that the closest correlation we can confirm w ould be the `sqft_basement`, however, the correlation is only valued at 0.17, not making it a good correlation. However, the other correlation we can look at is the `yr_built` which provides a negative correlation valued at -0.36, which although may be better, it is not good enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It appears to be difficult to note down which features are correlated to the condition of a house. This is highly due to the observed lack of samples for houses under condition 1, 2, and 5. A majority of the houses comprise of condition 3 and make it difficult to observe the correlation of the other conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Geospatial Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Map Breakdown"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this analysis, we will be looking at the location of the houses. We will be using the `lat` and `long` features to plot the location of the houses. We will identify if there are any geological patterns. Let's start by plotting a map of king county."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare geo data\n",
    "crs = {'init':'EPSG:4326'} # Coordinate Reference System (CRS) for the data\n",
    "geometry = [Point(xy) for xy in zip(df['long'], df['lat'])]\n",
    "geo_df = gpd.GeoDataFrame(df, \n",
    "                        crs = crs, \n",
    "                        geometry = geometry)\n",
    "\n",
    "kings_county_map = gpd.read_file('king_county/2010_Census_Tracts_for_King_County_-_Conflated_to_Parcels___tracts10_area.shp')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot to Latitude and Longitude points on the map\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "kings_county_map.to_crs(epsg=4326).plot(ax=ax, color='lightgrey')\n",
    "geo_df.plot(ax=ax, markersize=15, color='blue', marker='o', label='Houses')\n",
    "ax.set_title('King County Map')\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an idea of where the houses sold are plotted across King County. It seems that most houses are situated to the left of King County. It is also quite densely packed. There are quite a lot of houses spread out towards the left and bottom, we need to consider the water bodies in the area. We will now mark the houses that are near the water bodies based on the `waterfront` feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get houses with `waterfront`` value of 1\n",
    "waterfront = df[df['waterfront'] == 1]\n",
    "\n",
    "waterfront_geometry = [Point(xy) for xy in zip(waterfront['long'], waterfront['lat'])]\n",
    "waterfront_df = gpd.GeoDataFrame(waterfront,\n",
    "                                crs = crs,\n",
    "                                geometry = waterfront_geometry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the `waterfront` houses on the map\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "kings_county_map.to_crs(epsg=4326).plot(ax=ax, color='lightgrey')\n",
    "waterfront_df.plot(ax=ax, markersize=15, color='blue', marker='o', label='Waterfront Houses')\n",
    "ax.set_title('King County Map')\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.show()\n",
    "\n",
    "# Get number of houses with `waterfront` value of 1 and 0\n",
    "waterfront_value_counts = df['waterfront'].value_counts()\n",
    "waterfront_value_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see now that there definitely are rivers at the left side of the map. We find there to be 163 houses near the water bodies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Condition of Houses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After seeing the location of houses, let's see the condition of houses based on their location. We will color the houses based on their condition as well as add an alpha value to the color to see the density of houses in the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of houses based on condition colored from red to blue\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "kings_county_map.to_crs(epsg=4326).plot(ax=ax, color='lightgrey')\n",
    "geo_df[geo_df['condition'] == 1].plot(ax=ax, markersize=30, color='red', marker='o', label='1', alpha=1)\n",
    "geo_df[geo_df['condition'] == 2].plot(ax=ax, markersize=30, color='orange', marker='o', label='2', alpha=0.7)\n",
    "geo_df[geo_df['condition'] == 3].plot(ax=ax, markersize=10, color='yellow', marker='o', label='3', alpha=0.1)\n",
    "geo_df[geo_df['condition'] == 4].plot(ax=ax, markersize=10, color='green', marker='o', label='4', alpha=0.1)\n",
    "geo_df[geo_df['condition'] == 5].plot(ax=ax, markersize=5, color='blue', marker='o', label='5', alpha=0.1)\n",
    "ax.set_title('King County Map')\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The color and sizes of the houses conditions that are common have been decreased while the color and size for the ones that are rare are increased to improve the visualization. Due to the overhwelming number of yellows and greens, let's remove them in the next plot below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of houses based on condition colored from red to blue\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "kings_county_map.to_crs(epsg=4326).plot(ax=ax, color='lightgrey')\n",
    "geo_df[geo_df['condition'] == 1].plot(ax=ax, markersize=30, color='red', marker='o', label='1', alpha=1)\n",
    "geo_df[geo_df['condition'] == 2].plot(ax=ax, markersize=30, color='orange', marker='o', label='2', alpha=0.7)\n",
    "geo_df[geo_df['condition'] == 5].plot(ax=ax, markersize=5, color='blue', marker='o', label='5', alpha=0.1)\n",
    "ax.set_title('King County Map')\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now it appears that the middle of king county has a huge variety of house conditions as a good majority of the poor conditioned houses are situated there. However, the houses that are in good condition are also situated there.\n",
    "\n",
    "It can also be observed that the farther right you go, the lower the condition of the houses go as well. This may be due to the fact that the houses are farther away from the city center.\n",
    "\n",
    "When it came to houses at the top left, it appears that the houses are in good condition. This could be due to the fact that the houses are near the water bodies. This may be a factor in determining the condition of a house."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting of houses based on condition colored from red to blue\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "kings_county_map.to_crs(epsg=4326).plot(ax=ax, color='lightgrey')\n",
    "geo_df[geo_df['condition'] == 4].plot(ax=ax, markersize=10, color='green', marker='o', label='4', alpha=0.1)\n",
    "geo_df[geo_df['condition'] == 5].plot(ax=ax, markersize=5, color='blue', marker='o', label='5', alpha=0.1)\n",
    "ax.set_title('King County Map')\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upon further observation, we could confirm that the top left is where most of the well conditioned houses are located."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Zipcode heatmap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After looking at basic latitude and longitude of houses, let's look at the zipcode of houses. We will be using the `zipcode` feature to provide the heatmap of the houses while still using the `lat` and `long` features to plot the location of the houses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot houses based on `lat` and `long` with `zipcode` as the color\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "kings_county_map.to_crs(epsg=4326).plot(ax=ax, color='whitesmoke')\n",
    "geo_df.plot(ax=ax, markersize=15, column='zipcode', legend=False,\n",
    "            cmap='tab20', marker='o', label='Houses')\n",
    "ax.set_title('King County Map')\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an idea of what the zipcode segregation of houses are. Let's further analyze the zipcode of houses by looking at the condition of houses based on their zipcode. We will color the houses based on their zipcode and size them based on condition, as well as add an alpha value to the color to see the density of houses in the area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot houses based on `lat` and `long` with `zipcode` as the color with alpha as the condition\n",
    "fig, ax = plt.subplots(figsize=(10, 10))\n",
    "kings_county_map.to_crs(epsg=4326).plot(ax=ax, color='whitesmoke')\n",
    "geo_df.plot(ax=ax, column='zipcode', legend=False,\n",
    "            cmap='tab20', marker='o', label='Houses', markersize=geo_df['condition']*2, alpha=0.1)\n",
    "ax.set_title('King County Map')\n",
    "plt.legend(prop={'size': 10})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It becomes further apparent that the houses at the top left have a good condition. The houses at the bottom left however, have a poor condition. They also are near water bodies so it may not be a factor in determining the condition of a house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In summary,the following are the findings of the geospatial analysis:\n",
    "- Most houses are situated to the left of King County.\n",
    "- There are quite a lot of houses spread out towards the left and bottom, we need to consider the water bodies in the area.\n",
    "- There are 163 houses near the water bodies.\n",
    "- The middle of king county has a huge variety of house conditions as a good majority of the poor conditioned houses are situated there. However, the houses that are in good condition are also situated there.\n",
    "- The farther right you go, the lower the condition of the houses go as well. This may be due to the fact that the houses are farther away from the city center.\n",
    "- The top left is where most of the well conditioned houses are located.\n",
    "- The houses at the top left have a good condition. The houses at the bottom left however, have a poor condition. They also are near water bodies so it may not be a factor in determining the condition of a house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Feature Relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop one hot encoded columns\n",
    "corr_df = df.drop(['day_of_week_sold_0', \n",
    "    'day_of_week_sold_1', \n",
    "    'day_of_week_sold_2', \n",
    "    'day_of_week_sold_3', \n",
    "    'day_of_week_sold_4', \n",
    "    'day_of_week_sold_5', \n",
    "    'day_of_week_sold_6'], \n",
    "    axis=1)\n",
    "\n",
    "# Plot heatmap of features\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.heatmap(corr_df.corr(), annot=True, cmap='viridis')\n",
    "plt.title('Correlation of Features')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the top N correlations without duplicates\n",
    "N = 25\n",
    "corr = corr_df.corr().abs()\n",
    "upper_triangle = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "top_corr = upper_triangle.unstack().sort_values(ascending=False).head(N)\n",
    "\n",
    "print(\"Top\", N, \"Correlations:\")\n",
    "print(top_corr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Outlier Detection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion of EDA Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 6. Model Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initial Preprocessing Before Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are finished with our EDA, we can now drop the `date` column as well as the `id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(['date', 'id'], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our approach to modeling the `house_prices.csv` dataset, we've identified several key preprocessing steps to ensure optimal model performance. During the exploratory data analysis (EDA), we noted that the dataset exhibits a significant imbalance in the distribution of the 'condition' classes, our target variable. Such an imbalance can lead to a model that disproportionately favors the majority class, leading to poor performance on minority classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To address this, our first step will be to employ the `stratify` parameter in `train_test_split` from Scikit-learn. This will ensure that the distribution of classes in both the training and testing sets mirrors that of the overall dataset, allowing for a more balanced and representative model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop(['condition'], axis=1)\n",
    "y = df['condition']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED, stratify=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Furthermore, considering the diverse range of feature scales in our dataset, such as the price and the number of bedrooms, we recognize the importance of feature scaling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To counter this, we will implement scaling using `StandardScaler` from Scikit-learn. This will standardize our features to have a mean of zero and a unit variance, ensuring that all features contribute equally to the model's training process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model = LogisticRegression(\n",
    "    random_state=SEED, \n",
    "    max_iter=1000\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve of the model\n",
    "train_sizes, train_scores, val_scores = learning_curve(lr_model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Calculate mean and standard deviation of training and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training Score', color='blue')\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, val_scores_mean, label='Validation Score', color='red')\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.2, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "train_predictions = lr_model.predict(X_train_scaled)\n",
    "test_predictions = lr_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate errors\n",
    "train_error = 1 - accuracy_score(y_train, train_predictions)\n",
    "test_error = 1 - accuracy_score(y_test, test_predictions)\n",
    "\n",
    "# Print errors\n",
    "print(f\"Training Error: {train_error * 100:.2f}%\")\n",
    "print(f\"Test Error: {test_error * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = lr_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred, average='weighted')\n",
    "recall_test = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "# Print or log the evaluation metrics for the test set\n",
    "print(f'Test Accuracy: {accuracy_test:.4f}')\n",
    "print(f'Test Precision: {precision_test:.4f}')\n",
    "print(f'Test Recall: {recall_test:.4f}')\n",
    "print(f'Test F1 Score: {f1_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaÃ¯ve Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model = GaussianNB()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the learning curve of the model\n",
    "train_sizes, train_scores, val_scores = learning_curve(nb_model, X_train_scaled, y_train, cv=5, scoring='accuracy')\n",
    "\n",
    "# Calculate mean and standard deviation of training and validation scores\n",
    "train_scores_mean = np.mean(train_scores, axis=1)\n",
    "train_scores_std = np.std(train_scores, axis=1)\n",
    "val_scores_mean = np.mean(val_scores, axis=1)\n",
    "val_scores_std = np.std(val_scores, axis=1)\n",
    "\n",
    "# Plot learning curves\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_scores_mean, label='Training Score', color='blue')\n",
    "plt.fill_between(train_sizes, train_scores_mean - train_scores_std, train_scores_mean + train_scores_std, alpha=0.2, color='blue')\n",
    "\n",
    "plt.plot(train_sizes, val_scores_mean, label='Validation Score', color='red')\n",
    "plt.fill_between(train_sizes, val_scores_mean - val_scores_std, val_scores_mean + val_scores_std, alpha=0.2, color='red')\n",
    "\n",
    "plt.xlabel('Training Set Size')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Learning Curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions\n",
    "train_predictions = nb_model.predict(X_train_scaled)\n",
    "test_predictions = nb_model.predict(X_test_scaled)\n",
    "\n",
    "# Calculate errors\n",
    "train_error = 1 - accuracy_score(y_train, train_predictions)\n",
    "test_error = 1 - accuracy_score(y_test, test_predictions)\n",
    "\n",
    "# Print errors\n",
    "print(f\"Training Error: {train_error * 100:.2f}%\")\n",
    "print(f\"Test Error: {test_error * 100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = nb_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred, average='weighted')\n",
    "recall_test = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "# Print or log the evaluation metrics for the test set\n",
    "print(f'Test Accuracy: {accuracy_test:.4f}')\n",
    "print(f'Test Precision: {precision_test:.4f}')\n",
    "print(f'Test Recall: {recall_test:.4f}')\n",
    "print(f'Test F1 Score: {f1_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SMOTE will be used only on the training set to create synthetic instances of the underrepresented classes in 'condition'."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "smote = SMOTE(random_state=SEED)\n",
    "X_train_smote, y_train_smote = smote.fit_resample(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The third model we will be using is the `Random Forest Model`. This model is an ensemble learning method for classification and regression. It operates by constructing a multitude of decision trees at training time and outputting the class that is the mode of the classes or mean prediction of the individual trees. It is known to be the most accurate model for classification problems. Since we're trying to classify houses based on their condition, we will be using this model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators= 100,\n",
    "    max_depth= 10,\n",
    "    max_features= 'sqrt',\n",
    "    max_leaf_nodes= None,\n",
    "    max_samples= None,\n",
    "    min_samples_split= 2,\n",
    "    bootstrap= True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = rf_model.predict(X_train_scaled)\n",
    "test_predictions = rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Print the training and test errors\n",
    "train_error = 1 - accuracy_score(y_train, train_predictions)\n",
    "test_error = 1 - accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(f'Training Error: {train_error:.4f}')\n",
    "print(f'Test Error: {test_error:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_pred = rf_model.predict(X_test_scaled)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred, average='weighted')\n",
    "recall_test = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "# Print or log the evaluation metrics for the test set\n",
    "print(f'Test Accuracy: {accuracy_test:.4f}')\n",
    "print(f'Test Precision: {precision_test:.4f}')\n",
    "print(f'Test Recall: {recall_test:.4f}')\n",
    "print(f'Test F1 Score: {f1_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confusion Matrix and Classification Report\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can observe a large improvement in the scores of the training data and validation data. However, the data is not perfect yet. This might be because the hyperparameter initially used for the model is not the best hyperparameter for the model. Let's try to find the best hyperparameter for the model by tuning its parameters in the next section.\n",
    "\n",
    "It should also be noted that the `F1-scores` have improved over the non-SMOTE data. This means that the model is now better at predicting the condition of a house."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 7. Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NaÃ¯ve Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training, validation, and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_train_scaled, y_train, test_size=0.2, random_state=SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid_nb = {\n",
    "    'var_smoothing': np.logspace(0,-9, num=100)\n",
    "}\n",
    "nb_model_grid = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, cv=5, scoring='accuracy', verbose=1, n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_model_grid.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f'Best Score: {nb_model_grid.best_score_}')\n",
    "print(f'Best Parameters: {nb_model_grid.best_params_}')\n",
    "print(f'Best Estimator: {nb_model_grid.best_estimator_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predict on the test set\n",
    "y_test_pred = nb_model_grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Random Forest Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The method we will be using for hyperparameter tuning is the `Random Search` method. This method is a technique where random combinations of the hyperparameters are used to find the best solution for the built model. This method is similar to a grid search but is more efficient as it does not exhaustively search through all possible combinations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameter Searching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [5, 10, 15, 20, 25, 30],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_leaf_nodes': [None, 5, 10, 15, 20, 25, 30],\n",
    "    'max_samples': [None, 0.5, 0.75, 1.0],\n",
    "    'min_samples_split': [2, 5, 10, 15, 20],\n",
    "    'bootstrap': [True]\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=RandomForestClassifier(random_state=SEED),\n",
    "    param_distributions=param_grid,\n",
    "    n_iter=50,\n",
    "    scoring='accuracy',\n",
    "    n_jobs=-1,\n",
    "    cv=5,\n",
    "    verbose=1,\n",
    "    random_state=SEED\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_search.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(f'Best n_estimators: {best_model.get_params()[\"n_estimators\"]}')\n",
    "print(f'Best max_depth: {best_model.get_params()[\"max_depth\"]}')\n",
    "print(f'Best max_features: {best_model.get_params()[\"max_features\"]}')\n",
    "print(f'Best max_leaf_nodes: {best_model.get_params()[\"max_leaf_nodes\"]}')\n",
    "print(f'Best max_samples: {best_model.get_params()[\"max_samples\"]}')\n",
    "print(f'Best min_samples_split: {best_model.get_params()[\"min_samples_split\"]}')\n",
    "print(f'Best bootstrap: {best_model.get_params()[\"bootstrap\"]}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Sanity Check:** You should get:\n",
    "\n",
    "```\n",
    "Best n_estimators: 500\n",
    "Best max_depth: 30\n",
    "Best max_features: sqrt\n",
    "Best max_leaf_nodes: None\n",
    "Best max_samples: 1.0\n",
    "Best min_samples_split: 20\n",
    "Best bootstrap: True\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we compare our hyperparameter before and after tuning, provided is the table.\n",
    "| Parameters        \t| Before \t| After \t|\n",
    "|-------------------\t|--------\t|-------\t|\n",
    "| n_estimator       \t| 100    \t| 300   \t|\n",
    "| max_depth         \t| 10     \t| 10    \t|\n",
    "| max_features      \t| sqrt   \t| sqrt  \t|\n",
    "| max_leaf_nodes    \t| None   \t| 20    \t|\n",
    "| max_samples       \t| None   \t| 0.5   \t|\n",
    "| min_samples_split \t| 2      \t| 5     \t|\n",
    "| Bootstrap         \t| True   \t| True  \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our new hyperparameters, let's try running the model again."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Training (Hyperparameter Tuned)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tuned_rf_model = RandomForestClassifier(\n",
    "    n_estimators=500,\n",
    "    max_depth=30,\n",
    "    max_features='sqrt',\n",
    "    max_leaf_nodes=None,\n",
    "    max_samples=1.0,\n",
    "    min_samples_split=20,\n",
    "    bootstrap=True,\n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "tuned_rf_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_predictions = tuned_rf_model.predict(X_train_scaled)\n",
    "test_predictions = tuned_rf_model.predict(X_test_scaled)\n",
    "\n",
    "# Print the training and test errors\n",
    "train_error = 1 - accuracy_score(y_train, train_predictions)\n",
    "test_error = 1 - accuracy_score(y_test, test_predictions)\n",
    "\n",
    "print(f'Training Error: {train_error:.4f}')\n",
    "print(f'Test Error: {test_error:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model on the test set\n",
    "y_test_pred = tuned_rf_model.predict(X_test_scaled)\n",
    "\n",
    "accuracy_test = accuracy_score(y_test, y_test_pred)\n",
    "precision_test = precision_score(y_test, y_test_pred, average='weighted')\n",
    "recall_test = recall_score(y_test, y_test_pred, average='weighted')\n",
    "f1_test = f1_score(y_test, y_test_pred, average='weighted')\n",
    "\n",
    "# Print or log the evaluation metrics for the test set\n",
    "print(f'Test Accuracy: {accuracy_test:.4f}')\n",
    "print(f'Test Precision: {precision_test:.4f}')\n",
    "print(f'Test Recall: {recall_test:.4f}')\n",
    "print(f'Test F1 Score: {f1_test:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print the confusion matrix and classification report\n",
    "print(confusion_matrix(y_test, y_test_pred))\n",
    "print(classification_report(y_test, y_test_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Findings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model reached an `F1-score` of 0.6794 and the test error is at 0.2868. This is a small improvement from the model, however, we must take into consideration that this dataset is imbalanced. Condition 3 is very common, while conditions 1 and 2 are scarce. This may be a factor in the model's performance. This may also be why 1 and 2 are never predicted correctly. This may be due to the fact that the model is not trained enough to predict the rare conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 8. Model Selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Comparison"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Model     \t| Logistic Regression \t| Naive Bayes \t| Random Forest \t|\n",
    "|-----------\t|---------------------\t|-------------\t|---------------\t|\n",
    "| Data      \t|                     \t|             \t| SMOTE         \t|\n",
    "| Accuracy  \t|                     \t|             \t| 0.6281        \t|\n",
    "| Precision \t|                     \t|             \t| 0.6287        \t|\n",
    "| Recall    \t|                     \t|             \t| 0.6281        \t|\n",
    "| F1 Score  \t|                     \t|             \t| 0.6236        \t|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Naive Bayes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After using a `RandomSearchCV` to find the best hyperparameters for the model, we found the following hyperparameters to be the best for the model.\n",
    "\n",
    "`n_estimators`=300\n",
    "\n",
    "`max_depth`=10\n",
    "\n",
    "`max_features`='sqrt'\n",
    "\n",
    "`max_leaf_nodes`=20\n",
    "\n",
    "`max_samples`=0.5\n",
    "\n",
    "`min_samples_split`=5\n",
    "\n",
    "`bootstrap`=True\n",
    "\n",
    "`random_state`=SEED"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 9. Insights and Conclusions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Section 10. References"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
